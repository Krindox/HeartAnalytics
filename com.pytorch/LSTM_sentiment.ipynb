{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs prep....\n",
      "25000\n",
      "25000\n",
      "(25000, 1)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "model = Word2Vec.load('wv_sentiment_aclimdb_len50')\n",
    "POS_TRAIN_PATH = 'data/aclImdb/train/pos/'\n",
    "NEG_TRAIN_PATH = 'data/aclImdb/train/neg/'\n",
    "pos_files = os.listdir(POS_TRAIN_PATH)\n",
    "neg_files = os.listdir(NEG_TRAIN_PATH)\n",
    "print('Docs prep....')\n",
    "pos_lines = []\n",
    "neg_lines = []\n",
    "for file in pos_files:\n",
    "    with open(POS_TRAIN_PATH + file) as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines if line.strip()]\n",
    "        pos_lines.extend(lines)\n",
    "for file in neg_files:\n",
    "    with open(NEG_TRAIN_PATH + file) as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [line.strip() for line in lines if line.strip()]\n",
    "        neg_lines.extend(lines)\n",
    "\n",
    "docs = []\n",
    "docs.extend(pos_lines)\n",
    "docs.extend(neg_lines)\n",
    "\n",
    "pos_labels = np.zeros(len(pos_lines))\n",
    "neg_labels = np.ones(len(neg_lines))\n",
    "\n",
    "print(len(docs))\n",
    "docs = [re.sub('([0-9\\-|\\?!]+|(<br\\s?\\/?>))',' ',sent) for sent in docs]\n",
    "print(len(docs))\n",
    "df2 = pd.DataFrame(docs)\n",
    "labels2 = []\n",
    "labels2 = np.append(pos_labels, neg_labels)\n",
    "df2['labels'] = labels2\n",
    "df2 = df2.sample(frac=1)\n",
    "def word_to_tensor(word):\n",
    "    return model.wv[word]\n",
    "\n",
    "def word_to_tensor(word):\n",
    "    return model.wv[word]\n",
    "\n",
    "def sent_to_tensor(sent):\n",
    "    tokens = word_tokenize(sent)\n",
    "    tokens = list(filter(lambda x:x in model.wv.vocab,tokens))\n",
    "    arr = []\n",
    "    for token in tokens:\n",
    "        arr.append(word_to_tensor(token))\n",
    "    return np.array(arr)\n",
    "\n",
    "def  getRand():\n",
    "    index = random.randint(0,len(X_train)-1)\n",
    "    sent = df2.iloc[index][0]\n",
    "    label = df2.iloc[index][1]\n",
    "    return sent, label\n",
    "\n",
    "# df2.to_csv('data/aclImdb/processed_input_labelled3.csv')\n",
    "\n",
    "# pos_labels = np.zeros(len(pos_lines))\n",
    "# neg_labels = np.ones(len(neg_lines))\n",
    "\n",
    "X = df2.drop('labels', axis=1)\n",
    "X = np.array(X)\n",
    "print(X.shape)\n",
    "train_size = int(len(docs)*.80)\n",
    "X_train = X[:train_size]\n",
    "X_test = X[train_size:]\n",
    "y_train = df2['labels'][:train_size]\n",
    "y_test = df2['labels'][train_size:]\n",
    "\n",
    "# torch.save(rnn.state_dict(), 'rnn.pth') ######### uncomment later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def  getVal(index):\n",
    "#     index = random.randint(0,len(X_train)-1)\n",
    "    sent = df2.iloc[index][0]\n",
    "    label = df2.iloc[index][1]\n",
    "    return sent, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This film was a disaster from start to finish. Interspersed with performances from \"the next generation of beautiful losers\" are interviews with Bono and The Edge as well as the performers themselves. This leaves little time for the clips of Leonard Cohen himself, who towers over everyone else in the film with his commanding yet gentle presence, wisdom and humor. The rest are too busy trying to canonize him as St. Leonard or as some Old Testament prophet. Many of the performances are forgettable over interpretations (especially Rufus & Martha Wainright's) or bland under achievements. Only Beth Orton and Anthony got within striking distance of Leonard's own versions by using a little restraint. Annoying little pseudo avant garde gestures are sprinkled throughout the film  like out of focus superimpositions of red spheres over many of the concert and interview shots, shaky blurred camera work, use of digital delay on some of Leonard Cohen's comments (making it harder to hear what's being said) and a spooky, pretentious low drone under a lot of the interview segments (an attempt at added gravitas ). For the real thing, see the Songs From The Life Of documentary produced by the BBC in  . 1.0\n",
      "\n",
      "-2.8872  1.9643 -0.6532  ...  -3.6259 -3.5270 -1.0431\n",
      "-2.5894  0.9766 -1.6440  ...  -3.6613 -3.9402 -0.4635\n",
      "-2.5257  0.1776 -1.0817  ...   1.6080 -2.3879  0.8734\n",
      "          ...             â‹±             ...          \n",
      "-0.2675  0.3384 -0.3123  ...  -0.2625 -0.0156  0.0457\n",
      "-1.1260 -0.6207  1.3667  ...  -1.1518 -1.5701  1.0597\n",
      "-1.5898 -2.2167  1.4284  ...   0.1974 -0.5794  0.3724\n",
      "[torch.FloatTensor of size 220x50]\n",
      "\n",
      "\n",
      " 1\n",
      "[torch.DoubleTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent, label = getVal(0)\n",
    "print(sent, label)\n",
    "input = sent_to_tensor(sent)\n",
    "print(torch.Tensor(input))\n",
    "label = torch.from_numpy(np.array([label]))\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim , hidden_dim,  target_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (Variable(torch.zeros(1, 1, self.hidden_dim)), \n",
    "                Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "#         print('IN FORWARD PASS')\n",
    "#         embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(sentence.view(len(sentence), 1, -1), self.hidden)\n",
    "#         print('EMBED size:', embeds.size())\n",
    "#         print('LSTM out: ', lstm_out.size())\n",
    "#         print('LAST :', lstm_out[-1].size())\n",
    "#         print('RESIZED FOR LINEAR')\n",
    "#         print(lstm_out.view(len(sentence), -1).size())\n",
    "        tag_space = self.hidden2tag(lstm_out[-1])\n",
    "#         print('TAG SPACE', tag_space.size())\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "#         print('TAG SCORE size: ',tag_scores.size())\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstmmodel = LSTMTagger(50, 128, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.6690 -0.7179\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstmmodel(Variable(torch.Tensor(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 samples processed\n",
      "Avg loss after 0 samples  0.649126768112\n",
      "1000 samples processed\n",
      "Avg loss after 1000 samples  0.775575342577\n",
      "2000 samples processed\n",
      "Avg loss after 2000 samples  0.740419247064\n",
      "3000 samples processed\n",
      "Avg loss after 3000 samples  0.727292080674\n",
      "4000 samples processed\n",
      "Avg loss after 4000 samples  0.721381831708\n",
      "5000 samples processed\n",
      "Avg loss after 5000 samples  0.716623214522\n",
      "6000 samples processed\n",
      "Avg loss after 6000 samples  0.71216655955\n",
      "7000 samples processed\n",
      "Avg loss after 7000 samples  0.708805617718\n",
      "8000 samples processed\n",
      "Avg loss after 8000 samples  0.704903775143\n",
      "9000 samples processed\n",
      "Avg loss after 9000 samples  0.700057807592\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/virtualpy36/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 3\n",
    "\n",
    "lstm_model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, 2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(lstm_model.parameters(), lr=learning_rate)\n",
    "\n",
    "lstm_model.train()\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = []\n",
    "    for i in range(train_size):\n",
    "        sent, label = getVal(i)\n",
    "        \n",
    "        inputs = Variable(torch.from_numpy(sent_to_tensor(sent)))\n",
    "        target = Variable(torch.from_numpy(np.array([label])).long())\n",
    "        \n",
    "        lstm_model.zero_grad()\n",
    "        lstm_model.hidden = lstm_model.init_hidden()\n",
    "        \n",
    "        outputs = lstm_model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        epoch_loss.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 1000 == 0:\n",
    "            print(i , 'samples processed')\n",
    "            print('Avg loss after %d samples ' % i, np.average(epoch_loss))\n",
    "    losses.append(np.average(epoch_loss))\n",
    "    print('Avg Epoch Loss:', losses[epoch])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_line(model , line):\n",
    "    sent_tensor = Variable(torch.from_numpy(sent_to_tensor(line)))\n",
    "    model.eval()\n",
    "    model.hidden = model.init_hidden()\n",
    "    output = model(sent_tensor)\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(lstm_model, x_test, y_test):\n",
    "    correct = 0.0\n",
    "    for i in range(len(x_test)):\n",
    "        input = x_test[i]\n",
    "        target = y_test.iloc[i]\n",
    "#         print(input)\n",
    "#         print(target)\n",
    "#         break\n",
    "        output = test_line(lstm_model, input[0])\n",
    "        if output.topk(1)[1].data.numpy()[0][0] == target:\n",
    "            correct += 1\n",
    "#         if i % 100 == 0:\n",
    "#             print(correct*1.0/ (i+1))\n",
    "    print(correct*1.0/ len(x_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not even a complete epoch and we hit somewhere around 60%  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.595\n"
     ]
    }
   ],
   "source": [
    "evaluate(lstm_model, x_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
